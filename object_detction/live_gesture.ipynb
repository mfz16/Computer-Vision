{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "model_path=r\"d://google_models/gesture/gesture_recognizer.task\"\n",
    "model_asset_path=model_path\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    \n",
    "    print('gesture recognition result: {}'.format(result))\n",
    "    image=output_image.numpy_view()\n",
    "    #cv2.imshow('frame', image)\n",
    "    #print(image.shape)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    print(image_h, image_w)\n",
    "    for hand_landmarks in result.hand_landmarks:\n",
    "         for landmark in hand_landmarks.landmark:\n",
    "            #pass\n",
    "             x, y = int(landmark.x * image_w), int(landmark.y * image_h)\n",
    "             cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "    cv2.imshow('frame', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    num_hands=2,min_hand_detection_confidence=.4,result_callback=print_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        print(type(frame))\n",
    "        recognizer.recognize_async(mp_image,int(cap.get(cv2.CAP_PROP_POS_MSEC)))\n",
    "        print(\"fgfgfgfgfgfgfg\",rt)    \n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('No gesture recognition result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GestureRecognizerResult(gestures=[[Category(index=-1, score=0.6399022936820984, display_name='', category_name='None')], [Category(index=-1, score=0.8255902528762817, display_name='', category_name='Victory')]], handedness=[[Category(index=0, score=0.9240756630897522, display_name='Right', category_name='Right')], [Category(index=1, score=0.980106770992279, display_name='Left', category_name='Left')]], hand_landmarks=[[NormalizedLandmark(x=0.8426246643066406, y=0.22558583319187164, z=-1.5417626286762243e-07, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.8230472803115845, y=0.3340579569339752, z=-0.04671793058514595, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7583574056625366, y=0.4059206247329712, z=-0.05893031507730484, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6980618834495544, y=0.43930867314338684, z=-0.05980255454778671, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6554290652275085, y=0.44067633152008057, z=-0.05795971676707268, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6920155882835388, y=0.30922597646713257, z=-0.050098683685064316, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6023868918418884, y=0.339681476354599, z=-0.06002344563603401, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5516384840011597, y=0.35525521636009216, z=-0.061406444758176804, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5100046396255493, y=0.36028456687927246, z=-0.0625753328204155, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6891847848892212, y=0.2598854899406433, z=-0.024014471098780632, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6731865406036377, y=0.3828468918800354, z=-0.04300815984606743, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7100454568862915, y=0.41561758518218994, z=-0.03873318061232567, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7357417345046997, y=0.3957603871822357, z=-0.029985254630446434, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6950039267539978, y=0.2439054548740387, z=-0.00019011781841982156, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.6875196695327759, y=0.35842326283454895, z=-0.01908029615879059, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7168547511100769, y=0.37977704405784607, z=-0.013935546390712261, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7369303703308105, y=0.3638179898262024, z=-0.004590187221765518, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7041733264923096, y=0.24212265014648438, z=0.020356351509690285, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7031269669532776, y=0.3321385383605957, z=0.004046538379043341, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7292624711990356, y=0.35105496644973755, z=0.003909067716449499, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.7492213845252991, y=0.3376123309135437, z=0.010504387319087982, visibility=0.0, presence=0.0)], [NormalizedLandmark(x=0.448472261428833, y=0.8807737231254578, z=3.8660931522827013e-07, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5009928345680237, y=0.8299351930618286, z=-0.01149754598736763, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5190969705581665, y=0.7559682130813599, z=-0.019887233152985573, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5042356848716736, y=0.6926679611206055, z=-0.026418251916766167, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.48175203800201416, y=0.6613850593566895, z=-0.03248657286167145, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.49795377254486084, y=0.7189991474151611, z=-0.0346696637570858, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5251607298851013, y=0.6283490657806396, z=-0.048730723559856415, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5385774374008179, y=0.5740538835525513, z=-0.055729202926158905, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.5497949719429016, y=0.5326859951019287, z=-0.059010326862335205, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.45801571011543274, y=0.707207977771759, z=-0.032265301793813705, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4626050591468811, y=0.6042711734771729, z=-0.0452701561152935, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.46697166562080383, y=0.5374035239219666, z=-0.050829268991947174, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.46883735060691833, y=0.4912480115890503, z=-0.053994081914424896, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.43063899874687195, y=0.7090963125228882, z=-0.02829819545149803, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4497332274913788, y=0.646889328956604, z=-0.038769397884607315, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4697768986225128, y=0.6596807837486267, z=-0.03945513442158699, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.48009467124938965, y=0.6813787221908569, z=-0.03692909702658653, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.41521671414375305, y=0.7196036577224731, z=-0.025025995448231697, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4408075213432312, y=0.6836828589439392, z=-0.030879007652401924, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4584871530532837, y=0.699487566947937, z=-0.028817929327487946, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.46806469559669495, y=0.7207242250442505, z=-0.024850698187947273, visibility=0.0, presence=0.0)]], hand_world_landmarks=[[Landmark(x=0.07323731482028961, y=-0.010831058025360107, z=0.015135176479816437, visibility=0.0, presence=0.0), Landmark(x=0.058352526277303696, y=0.016909264028072357, z=-0.00280997552908957, visibility=0.0, presence=0.0), Landmark(x=0.035904157906770706, y=0.02774651348590851, z=0.0032939936500042677, visibility=0.0, presence=0.0), Landmark(x=0.009956438094377518, y=0.04059154912829399, z=0.004072092939168215, visibility=0.0, presence=0.0), Landmark(x=-0.008010071702301502, y=0.041863422840833664, z=0.010666402988135815, visibility=0.0, presence=0.0), Landmark(x=0.00017745787044987082, y=0.009661152958869934, z=-0.014153513126075268, visibility=0.0, presence=0.0), Landmark(x=-0.024229275062680244, y=0.015767280012369156, z=-0.01517074927687645, visibility=0.0, presence=0.0), Landmark(x=-0.04944043979048729, y=0.01792159676551819, z=-0.019049374386668205, visibility=0.0, presence=0.0), Landmark(x=-0.08369793742895126, y=0.02510172873735428, z=-0.03032890520989895, visibility=0.0, presence=0.0), Landmark(x=-0.002952603157609701, y=0.0004326910711824894, z=-0.00107694196049124, visibility=0.0, presence=0.0), Landmark(x=-0.004293522331863642, y=0.023700563237071037, z=0.005218952428549528, visibility=0.0, presence=0.0), Landmark(x=0.015040235593914986, y=0.04018716886639595, z=0.014310180209577084, visibility=0.0, presence=0.0), Landmark(x=0.021390656009316444, y=0.031420473009347916, z=0.020711073651909828, visibility=0.0, presence=0.0), Landmark(x=-0.0016271043568849564, y=-0.006543275900185108, z=0.008980736136436462, visibility=0.0, presence=0.0), Landmark(x=0.005948439706116915, y=0.018522415310144424, z=0.023686755448579788, visibility=0.0, presence=0.0), Landmark(x=0.016807112842798233, y=0.028883129358291626, z=0.03163843974471092, visibility=0.0, presence=0.0), Landmark(x=0.023699725046753883, y=0.0231443140655756, z=0.035212013870477676, visibility=0.0, presence=0.0), Landmark(x=0.009424533694982529, y=-0.007272654678672552, z=0.02575165592133999, visibility=0.0, presence=0.0), Landmark(x=0.013943903148174286, y=0.013435439206659794, z=0.03493177145719528, visibility=0.0, presence=0.0), Landmark(x=0.023280533030629158, y=0.021146055310964584, z=0.042807742953300476, visibility=0.0, presence=0.0), Landmark(x=0.03126240149140358, y=0.01724151335656643, z=0.035467009991407394, visibility=0.0, presence=0.0)], [Landmark(x=-0.00783158652484417, y=0.07573634386062622, z=0.058173101395368576, visibility=0.0, presence=0.0), Landmark(x=0.018649332225322723, y=0.05651403218507767, z=0.03517625108361244, visibility=0.0, presence=0.0), Landmark(x=0.033433228731155396, y=0.028146201744675636, z=0.030644766986370087, visibility=0.0, presence=0.0), Landmark(x=0.027069855481386185, y=-0.0010566432029008865, z=0.021545270457863808, visibility=0.0, presence=0.0), Landmark(x=0.013892937451601028, y=-0.017925919964909554, z=0.005982870701700449, visibility=0.0, presence=0.0), Landmark(x=0.019408967345952988, y=-9.890645742416382e-05, z=-0.007558508776128292, visibility=0.0, presence=0.0), Landmark(x=0.03501245379447937, y=-0.02238300070166588, z=-0.013364827260375023, visibility=0.0, presence=0.0), Landmark(x=0.0441531278192997, y=-0.04729805141687393, z=-0.0081672053784132, visibility=0.0, presence=0.0), Landmark(x=0.05004891753196716, y=-0.06322234869003296, z=0.0018224738305434585, visibility=0.0, presence=0.0), Landmark(x=0.0008596049156039953, y=-0.0023437845520675182, z=-0.0017911953618749976, visibility=0.0, presence=0.0), Landmark(x=0.0033085132017731667, y=-0.03889015316963196, z=-0.003905643243342638, visibility=0.0, presence=0.0), Landmark(x=0.0011445078998804092, y=-0.06349874287843704, z=-0.004051185678690672, visibility=0.0, presence=0.0), Landmark(x=0.007894840091466904, y=-0.08726812899112701, z=0.006838505156338215, visibility=0.0, presence=0.0), Landmark(x=-0.013582669198513031, y=-0.001157999038696289, z=0.005100733134895563, visibility=0.0, presence=0.0), Landmark(x=-0.008090389892458916, y=-0.02224412001669407, z=0.01841893419623375, visibility=0.0, presence=0.0), Landmark(x=0.0029504578560590744, y=-0.014935163781046867, z=0.04202311113476753, visibility=0.0, presence=0.0), Landmark(x=0.0089503712952137, y=-0.0057372297160327435, z=0.057490184903144836, visibility=0.0, presence=0.0), Landmark(x=-0.029865680262446404, y=0.005248601548373699, z=0.023168249055743217, visibility=0.0, presence=0.0), Landmark(x=-0.01780788227915764, y=-0.00586030725389719, z=0.03726554289460182, visibility=0.0, presence=0.0), Landmark(x=-0.00458046980202198, y=-0.0044938987120985985, z=0.0539107583463192, visibility=0.0, presence=0.0), Landmark(x=-0.00017882767133414745, y=0.006669335998594761, z=0.057941459119319916, visibility=0.0, presence=0.0)]])\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\VS_code\\newpy\\object_detction\\live_gesture.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(gesture_recognition_result)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m'\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m'\u001b[39;49m, mp_image)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - mat is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::cuda::GpuMat> for argument 'mat'\n>  - Expected Ptr<cv::UMat> for argument 'mat'\n"
     ]
    }
   ],
   "source": [
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    # Load the input image from an image file.\n",
    "    mp_image = mp.Image.create_from_file('d://Downloads/download (2).jpeg')\n",
    "\n",
    "    # Load the input image from a numpy array.\n",
    "    #mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_image)\n",
    "    # Perform gesture recognition on the provided single image.\n",
    "# The gesture recognizer must be created with the image mode.\n",
    "    gesture_recognition_result = recognizer.recognize(mp_image)\n",
    "print(gesture_recognition_result)\n",
    "import cv2\n",
    "cv2.imshow('frame', mp_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gesture recognition result: GestureRecognizerResult(gestures=[[Category(index=-1, score=0.9327282309532166, display_name='', category_name='None')]], handedness=[[Category(index=0, score=0.9916017651557922, display_name='Right', category_name='Right')]], hand_landmarks=[[NormalizedLandmark(x=0.05317072570323944, y=0.6190474033355713, z=7.113678748282837e-07, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.07488740980625153, y=0.48567870259284973, z=0.00937027670443058, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.14410501718521118, y=0.3946431875228882, z=0.002297075232490897, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.22256046533584595, y=0.36972442269325256, z=-0.007213825359940529, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.28106194734573364, y=0.3846921920776367, z=-0.015040910802781582, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.1500682234764099, y=0.32699012756347656, z=-0.02792034111917019, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.283685564994812, y=0.29325205087661743, z=-0.048316191881895065, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.36140698194503784, y=0.2968204915523529, z=-0.052414506673812866, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.41523227095603943, y=0.30496692657470703, z=-0.051426950842142105, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.174592062830925, y=0.37015312910079956, z=-0.043395355343818665, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.32783252000808716, y=0.34434759616851807, z=-0.05897662043571472, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4136340022087097, y=0.3599229156970978, z=-0.0587373785674572, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.47164320945739746, y=0.3810897767543793, z=-0.05560058355331421, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.20070454478263855, y=0.4353879392147064, z=-0.05725898966193199, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.3400322198867798, y=0.40002861618995667, z=-0.06894116848707199, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.4191269278526306, y=0.4093124270439148, z=-0.07147350162267685, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.47330644726753235, y=0.4249357283115387, z=-0.0719318762421608, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.22472842037677765, y=0.5173764228820801, z=-0.06975501775741577, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.3196907043457031, y=0.4862576425075531, z=-0.07615800201892853, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.3683739900588989, y=0.48279228806495667, z=-0.0743446946144104, visibility=0.0, presence=0.0), NormalizedLandmark(x=0.40497368574142456, y=0.4801219403743744, z=-0.07139410078525543, visibility=0.0, presence=0.0)]], hand_world_landmarks=[[Landmark(x=-0.059174325317144394, y=0.050930455327034, z=0.0397021546959877, visibility=0.0, presence=0.0), Landmark(x=-0.04639844968914986, y=0.022575093433260918, z=0.0424400195479393, visibility=0.0, presence=0.0), Landmark(x=-0.023799652233719826, y=0.007547598332166672, z=0.049654122442007065, visibility=0.0, presence=0.0), Landmark(x=-0.0008645132766105235, y=-0.003878830000758171, z=0.05427580326795578, visibility=0.0, presence=0.0), Landmark(x=0.022204067558050156, y=-0.0014502955600619316, z=0.043851520866155624, visibility=0.0, presence=0.0), Landmark(x=-0.008438218384981155, y=-0.01722712628543377, z=0.008982817642390728, visibility=0.0, presence=0.0), Landmark(x=0.020078476518392563, y=-0.022091368213295937, z=0.011363697238266468, visibility=0.0, presence=0.0), Landmark(x=0.0436093807220459, y=-0.023639746010303497, z=0.022569015622138977, visibility=0.0, presence=0.0), Landmark(x=0.060426436364650726, y=-0.02309141308069229, z=0.03556410223245621, visibility=0.0, presence=0.0), Landmark(x=-0.0011157735716551542, y=-0.0052178543992340565, z=0.0005697517190128565, visibility=0.0, presence=0.0), Landmark(x=0.035811346024274826, y=-0.010463685728609562, z=-0.0005203711334615946, visibility=0.0, presence=0.0), Landmark(x=0.058267656713724136, y=-0.013957185670733452, z=0.007132997736334801, visibility=0.0, presence=0.0), Landmark(x=0.08469744026660919, y=-0.008248746395111084, z=0.02602076530456543, visibility=0.0, presence=0.0), Landmark(x=0.005738737527281046, y=0.00952158309519291, z=-0.007111934944987297, visibility=0.0, presence=0.0), Landmark(x=0.03619915246963501, y=0.0022746799513697624, z=-0.006138139870017767, visibility=0.0, presence=0.0), Landmark(x=0.061567407101392746, y=0.002451712265610695, z=0.0009253034368157387, visibility=0.0, presence=0.0), Landmark(x=0.08137167245149612, y=0.007236182689666748, z=0.01791604608297348, visibility=0.0, presence=0.0), Landmark(x=0.0041047059930861, y=0.025034993886947632, z=-0.0031956082675606012, visibility=0.0, presence=0.0), Landmark(x=0.02701392024755478, y=0.02355341985821724, z=-0.0011254365090280771, visibility=0.0, presence=0.0), Landmark(x=0.04804389178752899, y=0.02256481721997261, z=0.002490205457434058, visibility=0.0, presence=0.0), Landmark(x=0.06033778563141823, y=0.01954175904393196, z=0.012956550344824791, visibility=0.0, presence=0.0)]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'left_hand_landmarks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\VS_code\\newpy\\object_detction\\live_gesture.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m gesture_results \u001b[39m=\u001b[39m recognizer\u001b[39m.\u001b[39mrecognize_async(mp_image,\u001b[39mint\u001b[39m(cap\u001b[39m.\u001b[39mget(cv2\u001b[39m.\u001b[39mCAP_PROP_POS_MSEC)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Draw landmarks and connections  \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m mp_drawing\u001b[39m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     image, \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     gesture_results\u001b[39m.\u001b[39;49mleft_hand_landmarks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     mp_hands\u001b[39m.\u001b[39mHAND_CONNECTIONS, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     mp_drawing\u001b[39m.\u001b[39mDrawingSpec(color\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m255\u001b[39m), thickness\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, circle_radius\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Check if there are recognized gestures\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m gesture_results\u001b[39m.\u001b[39mgestures:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/live_gesture.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# Print and display recognized gestures\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'left_hand_landmarks'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp \n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Gesture recognizer\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "\n",
    "model_path=r\"d://google_models/gesture/gesture_recognizer.task\"\n",
    "model_asset_path=model_path\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print('gesture recognition result: {}'.format(result))\n",
    "    \n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    num_hands=2,min_hand_detection_confidence=.4,result_callback=print_result)\n",
    "# Video capture \n",
    "cap = cv2.VideoCapture(0)\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the image horizontally for natural interface\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert OpenCV BGR to RGB\n",
    "        image.flags.writeable = False\n",
    "        mp_image=mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        # Make detection\n",
    "        gesture_results = recognizer.recognize_async(mp_image,int(cap.get(cv2.CAP_PROP_POS_MSEC)))\n",
    "            \n",
    "        # Draw landmarks and connections  \n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            gesture_results.left_hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS, \n",
    "            mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=4),\n",
    "        )\n",
    "\n",
    "        # Check if there are recognized gestures\n",
    "        if gesture_results.gestures:\n",
    "            \n",
    "            # Print and display recognized gestures\n",
    "            for gesture in gesture_results.gestures:\n",
    "                print(gesture)\n",
    "                mp_drawing.draw_landmarks(image, gesture.landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Show the final output image    \n",
    "        cv2.imshow('Gesture Recognition', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "model_path = r\"d://google_models/gesture/gesture_recognizer.task\"\n",
    "model_asset_path = model_path\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print('gesture recognition result: {}'.format(result))\n",
    "    image = output_image.numpy_view()\n",
    "    mp.drawing.draw_landmarks(output_image, result.hand_landmarks, mp.tasks.vision.HAND_CONNECTIONS)\n",
    "    #if result.gesture_type != mp.tasks.vision.GestureType.UNKNOWN:\n",
    "        #mp.drawing.draw_label(output_image, result.gesture_type.name, result.gesture_center, mp.drawing.LABEL_FONT_CURSIVE)\n",
    "    #image_h, image_w, _ = image.shape\n",
    "\n",
    "    # Draw hand landmarks on the image\n",
    "    #for hand_landmarks in result.hand_landmarks:\n",
    "        #for landmark in hand_landmarks.landmark:\n",
    "            #x, y = int(landmark.x * image_w), int(landmark.y * image_h)\n",
    "            #cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the image with hand landmarks\n",
    "    #cv2.imshow('frame_with_landmarks', cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    num_hands=2, min_hand_detection_confidence=0.4, result_callback=print_result)\n",
    "\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "        \n",
    "        # Recognize hand gestures asynchronously\n",
    "        result=recognizer.recognize_async(mp_image, int(cap.get(cv2.CAP_PROP_POS_MSEC)))\n",
    "        #print(result.hand_landmarks)\n",
    "\n",
    "        # Display the original frame\n",
    "        cv2.imshow('original_frame', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print('No gesture recognition result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Initialize MediaPipe Drawing module\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get hand gesture information\n",
    "            # You can customize this part based on your specific gestures\n",
    "            # For example, you can check the position of specific landmarks to determine gestures\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Detection', frame)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
