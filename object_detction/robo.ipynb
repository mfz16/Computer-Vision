{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\ProgramData\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pybullet as p\n",
    "import time\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Disconnect from the existing PyBullet GUI connection if it exists\n",
    "try:\n",
    "    p.disconnect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Connect to the PyBullet physics server with a new GUI connection\n",
    "physicsClient = p.connect(p.GUI)\n",
    "\n",
    "# Set gravity in the simulation\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Load a simple hand model into PyBullet\n",
    "hand_urdf_path = r'd://urdf_files/hand/urdf/shadow_hand.urdf'  # Replace with the actual path\n",
    "hand_id = p.loadURDF(hand_urdf_path, useFixedBase=True)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Get the coordinates of the tip of the index finger (landmark index 8)\n",
    "        index_finger_tip = hand_landmarks.landmark[8]\n",
    "        index_finger_x, index_finger_y = index_finger_tip.x, index_finger_tip.y\n",
    "\n",
    "        # Update PyBullet hand position based on the detected coordinates\n",
    "        p.resetBasePositionAndOrientation(hand_id, [index_finger_x, index_finger_y, 1.0], [0, 0, 0, 1])\n",
    "\n",
    "    # Step the PyBullet simulation\n",
    "    p.stepSimulation()\n",
    "\n",
    "    # Render the simulation\n",
    "    p.getCameraImage(800, 600)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Simulation', frame)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Disconnect from the PyBullet physics server\n",
    "p.disconnect()\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pybullet as p\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Disconnect from the existing PyBullet GUI connection if it exists\n",
    "try:\n",
    "    p.disconnect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Connect to the PyBullet physics server with a new GUI connection\n",
    "physicsClient = p.connect(p.GUI)\n",
    "\n",
    "# Set gravity in the simulation\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Load a simple hand model into PyBullet\n",
    "hand_urdf_path = \"d://urdf_files/hand/urdf/shadow_hand.urdf\"  # Replace with the actual path\n",
    "hand_id = p.loadURDF(hand_urdf_path, useFixedBase=True)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Finger joint indices in PyBullet (adjust these based on your URDF file)\n",
    "finger_joint_indices = [0, 1, 2, 3, 4,6,7,8,9,10]  # Adjust based on your URDF file\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Control forearm-wrist joint based on the detected coordinates\n",
    "        wrist_joint_angle = np.interp(hand_landmarks.landmark[8].y, [0, 1], [-0.5236, 0.1745])\n",
    "        p.setJointMotorControl2(hand_id, jointIndex=0, controlMode=p.POSITION_CONTROL, targetPosition=wrist_joint_angle)\n",
    "\n",
    "        # Control finger joints based on the detected coordinates\n",
    "        for i, joint_index in enumerate(finger_joint_indices):\n",
    "            finger_joint_angle = np.interp(hand_landmarks.landmark[i + 2].y, [0, 1], [-0.5236, 0.1745])\n",
    "            p.setJointMotorControl2(hand_id, jointIndex=joint_index, controlMode=p.POSITION_CONTROL, targetPosition=finger_joint_angle)\n",
    "\n",
    "    # Step the PyBullet simulation\n",
    "    p.stepSimulation()\n",
    "\n",
    "    # Render the simulation\n",
    "    p.getCameraImage(800, 600)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Simulation', frame)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Disconnect from the PyBullet physics server\n",
    "p.disconnect()\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "GetJointInfo failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\VS_code\\newpy\\object_detction\\robo.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m joint_name, landmark_index \u001b[39min\u001b[39;00m joint_to_landmark_mapping\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     joint_angle \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minterp(hand_landmarks\u001b[39m.\u001b[39mlandmark[landmark_index]\u001b[39m.\u001b[39my, [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m-\u001b[39m\u001b[39m1.047\u001b[39m, \u001b[39m1.047\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     joint_info \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mgetJointInfo(hand_id, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# Use -1 to get information for all joints\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     joint_index \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(j[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m joint_info \u001b[39mif\u001b[39;00m j[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdecode() \u001b[39m==\u001b[39m joint_name\u001b[39m.\u001b[39mencode())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     p\u001b[39m.\u001b[39msetJointMotorControl2(hand_id, jointIndex\u001b[39m=\u001b[39mjoint_index, controlMode\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mPOSITION_CONTROL, targetPosition\u001b[39m=\u001b[39mjoint_angle)\n",
      "\u001b[1;31merror\u001b[0m: GetJointInfo failed."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pybullet as p\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hand module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Disconnect from the existing PyBullet GUI connection if it exists\n",
    "try:\n",
    "    p.disconnect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Connect to the PyBullet physics server with a new GUI connection\n",
    "physicsClient = p.connect(p.GUI)\n",
    "\n",
    "# Set gravity in the simulation\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Load a simple hand model into PyBullet\n",
    "hand_urdf_path = \"d://urdf_files/hand/urdf/shadow_hand.urdf\"  # Replace with the actual path\n",
    "hand_id = p.loadURDF(hand_urdf_path, useFixedBase=True)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define a mapping between joint names and corresponding MediaPipe landmarks\n",
    "joint_to_landmark_mapping = {\n",
    "    \"thumb_joint1\": 2,  # Replace with the actual landmark index for thumb_joint1\n",
    "    \"thumb_joint2\": 3,  # Replace with the actual landmark index for thumb_joint2\n",
    "    \"thumb_joint3\": 4   # Replace with the actual landmark index for thumb_joint3\n",
    "}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hands\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Check if hands are detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "        # Control thumb joints based on the detected coordinates\n",
    "        for joint_name, landmark_index in joint_to_landmark_mapping.items():\n",
    "            joint_angle = np.interp(hand_landmarks.landmark[landmark_index].y, [0, 1], [-1.047, 1.047])\n",
    "            joint_info = p.getJointInfo(hand_id, -1)  # Use -1 to get information for all joints\n",
    "            joint_index = next(j[0] for j in joint_info if j[1].decode() == joint_name.encode())\n",
    "            p.setJointMotorControl2(hand_id, jointIndex=joint_index, controlMode=p.POSITION_CONTROL, targetPosition=joint_angle)\n",
    "\n",
    "    # Step the PyBullet simulation\n",
    "    p.stepSimulation()\n",
    "\n",
    "    # Render the simulation\n",
    "    p.getCameraImage(800, 600)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Gesture Simulation', frame)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Disconnect from the PyBullet physics server\n",
    "p.disconnect()\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\VS_code\\newpy\\object_detction\\robo.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W3sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m frame_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W3sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m result \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39mprocess(frame_rgb)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W3sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m hand_landmarks \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mmulti_hand_landmarks[\u001b[39m0\u001b[39;49m]  \u001b[39m# Assuming only one hand\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W3sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# Control PyBullet hand based on hand gestures\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_code/newpy/object_detction/robo.ipynb#W3sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m thumb_angles \u001b[39m=\u001b[39m map_thumb(hand_landmarks)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pybullet as p\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Disconnect from the existing PyBullet GUI connection if it exists\n",
    "try:\n",
    "    p.disconnect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Connect to the PyBullet physics server with a new GUI connection\n",
    "physicsClient = p.connect(p.GUI)\n",
    "\n",
    "# Set gravity in the simulation\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Load a simple hand model into PyBullet\n",
    "hand_urdf_path = \"d://urdf_files/hand/urdf/shadow_hand.urdf\"  # Replace with the actual path\n",
    "hand_id = p.loadURDF(hand_urdf_path, useFixedBase=True)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Landmark indices mapping\n",
    "landmarks_mapping = {\n",
    "    \"Wrist\": 0,\n",
    "    \"Thumb_Base\": 1,\n",
    "    \"Thumb_Knuckle\": 2,\n",
    "    \"Thumb_Middle\": 3,\n",
    "    \"Thumb_Tip\": 4,\n",
    "    \"Index_Knuckle\": 5,\n",
    "    \"Index_Middle\": 6,\n",
    "    \"Index_Tip\": 7,\n",
    "    \"Middle_Knuckle\": 9,\n",
    "    \"Middle_Middle\": 10,\n",
    "    \"Middle_Tip\": 11,\n",
    "    \"Ring_Knuckle\": 13,\n",
    "    \"Ring_Middle\": 14,\n",
    "    \"Ring_Tip\": 15,\n",
    "    \"Pinky_Knuckle\": 17,\n",
    "    \"Pinky_Middle\": 18,\n",
    "    \"Pinky_Tip\": 19,\n",
    "    \"Palm_Base\": 0,\n",
    "    \"Palm_Center\": 12\n",
    "}\n",
    "\n",
    "# Function to map hand landmarks to joint angles for the thumb\n",
    "def map_thumb(hand_landmarks):\n",
    "    thumb_base_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Thumb_Base\"]])\n",
    "    thumb_knuckle_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Thumb_Knuckle\"]])\n",
    "    thumb_middle_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Thumb_Middle\"]])\n",
    "    thumb_tip_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Thumb_Tip\"]])\n",
    "    return thumb_base_angle, thumb_knuckle_angle, thumb_middle_angle, thumb_tip_angle\n",
    "\n",
    "# Function to map hand landmarks to joint angles for the index finger\n",
    "def map_index(hand_landmarks):\n",
    "    index_knuckle_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Index_Knuckle\"]])\n",
    "    index_middle_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Index_Middle\"]])\n",
    "    index_tip_angle = calculate_joint_angle(hand_landmarks.landmark[landmarks_mapping[\"Index_Tip\"]])\n",
    "    return index_knuckle_angle, index_middle_angle, index_tip_angle\n",
    "\n",
    "# Similar functions for other fingers...\n",
    "\n",
    "# PyBullet simulation loop\n",
    "while True:\n",
    "    # Get hand landmarks from webcam\n",
    "    _, frame = cap.read()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "    hand_landmarks = result.multi_hand_landmarks[0]  # Assuming only one hand\n",
    "\n",
    "    # Control PyBullet hand based on hand gestures\n",
    "    thumb_angles = map_thumb(hand_landmarks)\n",
    "    set_joint_angles(\"Thumb\", thumb_angles)\n",
    "\n",
    "    index_angles = map_index(hand_landmarks)\n",
    "    set_joint_angles(\"Index\", index_angles)\n",
    "\n",
    "    # Similar calls for other fingers...\n",
    "\n",
    "    # Step the PyBullet simulation\n",
    "    p.stepSimulation()\n",
    "\n",
    "    # Display the PyBullet simulation\n",
    "    # (you may need additional code to visualize the simulated hand)\n",
    "\n",
    "    # Break the loop on some condition (e.g., press 'q' to quit)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#calculate angles in 3d space\n",
    "\n",
    "def calculate_joint_angle(landmark):\n",
    "    x, y ,z= landmark.x, landmark.y,landmark.z\n",
    "    return np.arctan2(z,np.sqrt(x**2+y**2))\n",
    "\n",
    "\n",
    "#define set_joint_angle_function\n",
    "def set_joint_angles(joint_name, angles):\n",
    "    joint_index = p.getJointInfo(hand_id, p.getJointIndex(hand_id, joint_name))[0]\n",
    "    p.setJointMotorControl2(hand_id, joint_index, p.POSITION_CONTROL, targetPosition=angles)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
